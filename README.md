# Summary
Code generation is the task of parsing natural language statements into source code like Python. Neural code generation models like TranX \cite{yin2018tranx} generate hypotheses by beam search and output the code with highest score, which however is not necessarily the best one. In this work, we propose a re-rank based framework which aims at re-ranking these hypothesis candidates. A classifier is trained to re-rank candidates using neural based and hand-crafted features. While our re-rank framework can be easily generalized to other code generation models or natural language related tasks such as machine translation, we extend TranX with our framework and demonstrate its effectiveness on CoNaLa \cite{yin2018learning} and Django \cite{oda2015ase:pseudogen1} dataset. Experiments show that our LeToRr framework improves the SOTA performance by **+3.4** ACC and **+4.6** BLEU on Django dataset.

# Introduction
Code generation is the task of parsing natural language utterances into source code written in general programming language, {\em e.g.}, Python.
Natural language to code (NL2Code) is a challenging NLP task for several reasons:
1. Natural language (NL) as the input is highly unstructured, while source code as the output needs to be well-structured. One can describe the intention in NL in different ways. There can also be several corresponding source code snippets to achieve the same intention.
2. It is non-trivial to model the semantics and underlying knowledge of a general programming language.
3. Neural model training is data hungry. While it is easy to obtain natural language intentions or code snippets, it is hard to obtain (NL intention, code snippet) pairs that have one to one correspondence.

Recently, several neural based approaches have been proposed to solve the code generation task. A brief survey of these models are covered in Section \ref{sec:related}, and detailed literature review is in Appendix.
Among them, TranX \cite{yin2018tranx} is a transition-based neural abstract syntax parser that achieves state-of-the-art results on several code generation tasks.
It enforces the output to be well-formed by representing code as ASTs and using neural networks to generate action sequences to construct such ASTs.
TranX and many other neural code generation models generate hypotheses by beam search and output the generated code with highest score.
However, the hypothesis that the model has most confidence in is not necessarily the best one.
For example, as shown in Tabel \ref{table:blue_gap}, we observe that for hypotheses generated by TranX, the top-k (oracle) hypotheses BLEU score can be 12\% higher than the top-1 BLEU score on CoNaLa dataset.
By re-ranking these hypotheses candidates, it is expected to obtain a better result.

|             | CoNaLa |
|-------------|--------|
| BLEU        | 23.99  |
| Oracle BLEU | 35.55  |

<p align="center">
<h4 align="center"> Table 1. Gap between top-1 and oracle on Conala dataset using TranX </h4>
</p>

|                 | Django |
|-----------------|--------|
| Accuracy        | 76.73  |
| Oracle Accuracy | 83.10  |
| BLEU            | 83.49  |
| Oracle BLEU     | 88.17  |

<p align="center">
<h4 align="center"> Table 2. Gap between top-1 and oracle on Django dataset us- ing TranX </h4>
</p>

In this paper, we propose a re-rank based framework which aims at re-ranking these hypothesis candidates.Besides the main neural model that generates code snippets given NL input, we also use neural models to annotate code with NL, compute the text similarity between NL input and code snippet, as well as the likelihood of a code snippet. We also use hand-crafted features such as code / action length prediction. A classifier is then trained to re-rank candidates using these neural based and hand-crafted features described above.
We cover the details of our re-rank based framework in Section \ref{sec:rerank}.

Our re-rank based framework has the following properties:
- **Generalization ability**: while we only extend TranX with our framework, it is a general ranking framework that can be easily generalized to other code generation models or related tasks such as machine translation.
- **Extensibility**: our framework can be extended by incorporating more features and potentially achieving better performance.
- **Effectiveness**: we demonstrate its effectiveness by achieving state-of-the-art results on CoNaLa \cite{yin2018learning} and Django \cite{oda2015ase:pseudogen1} dataset, which is detailed in Section \ref{sec:exp}.
\end{itemize}

Considering the data hungry problem, we also explored data augmentation approaches such as bootstrapping, which is covered in Section 4.

# Related Work
## Semantic Parsing
Semantic parsing aims at translating natural language (NL) into machine-executable logical forms or programs, and thus are key components in NL understanding systems. The task of code synthesis or code generation can be considered as a type of semantic parsing with the goal to map sentences in natural language to a formal representation with the underlying meaning (Mooney, 2014). In the last two years, the models used for semantic parsing have changed dramatically, with the introduction of neural methods that allow for rethinking a number of the previous assumptions underlying semantic parsing. Especially, sequence-to-sequence (seq2seq) models have achieved state-of-the-art results in code generation. These work include (Jia and Liang, 2016), (Lo- cascio et al., 2016), (Ling et al., 2016), (Iyer et al., 2017), (Zhong et al., 2017), (Rabinovich et al., 2017), (Lin et al., 2017). On the other hand, different traditional sequence model, another direction is to use tree structure to rep- resent code with the hope to incorporate the syntactical knowledge of the code (Hayati et al., 2018) (Rabinovich et al., 2017) (Yin and Neubig, 2017) . Following this direction, Yin and Neubig (2018) proposed a transition based framework TranX, which is the SOTA method of neural code generation of Python. TranX is transition based neural semantic parser that maps NL utterances into formal meaning representations (MRs). The key idea of TranX is that it uses a transition system based on the abstract syntax description language (ASDL) for the target MR. This brings two advantages: first, the grammar model could explicitly capture the target syntax as prior knowledge, which can help constrain the output space and model the information flow, and second, it is highly generalizable, and can easily be applied to any other type. However, it is not trivial to represent code as a tree due to the significant number of nodes in the tree compared to the length of the NL description. Based on this observation, Hayati et al. (2018) proposed a framework RECODE, which is an adaptation of retrieval-based neural machine translation method (Zhang et al., 2018) to
the code generation problem by expanding it to apply to generation of tree structures. Recently, (Dong and Lapata, 2018) proposed a Coarse-to-Fine framework which first generates a rough sketch of meaning of input natural language utterance, and then fills in details. Their model is regarded as current state-of-the-art approach in code generation.

## Learning to Rank
Learning to rank (LoTR) has been applied to a wide range of applications in Natural Language Processing (NLP), Data Mining (DM) and Information Retrieval (IR). It has demonstrated its effectiveness in document retrieval, collaborative filtering, document summarization, machine translation, and question answering \cite{li2011learning}.  In the context of document retrieval, given a query, the system which maintains a collection of documents, retrieves documents relevant to the query from the collection, ranks the documents, and returns the top ranked documents. The ranking task is performed by using a ranking model $f(q,d)$ to sort the documents, where $q$ denotes a query and $d$ denotes a document. Current trend is to utilize machine learning techniques to automatically construct the ranking model $f(q, d)$. Similarly, in the context of code generation, the candidate hypothesis $h$ is analogous to document, and the given natural language $n$ is analogous to the query, so the ranking model in code generation is formulated as $f(h, n)$. 

**Pointwise Approach**: In the pointwise approach, the ranking problem is converted to regression or ordinal classification, where the group structure of ranking is therefore ignored. The pointwise approach includes Subset Ranking McRank (Li et al., 2008), (Cossock and Zhang, 2006), Prank (Crammer and Singer, 2002), and OC SVM (Shashua and Levin, 2003).

**Pairwise Approach**: In the pairwise approach, ranking is transformed into pairwise classification or pairwise regression. It only cares about the relative order of each pair and thus ignores the global ordering. In terms of the classification, a classifier (normally a binary classifier) for classifying the ranking orders of pairs is created and is employed in the ranking of documents. The pairwise approach includes Ranking SVM (Herbrich, 2000), RankBoost (Freund et al., 2003), RankNet (Burges et al., 2005), GBRank (Zheng et al., 2008), IR SVM (Cao et al., 2006), LambdaRank (Burges et al., 2007), and LambdaMART (Wu et al., 2010).

**Listwise Approach**: The listwise approach addresses the ranking problem in a more straightforward way. Specifically, it takes ranking lists as instances in both learning and prediction. The group structure of ranking is maintained and ranking evaluation measures can be more directly incorporated into the loss functions in learning. The listwise approach includes ListNet (Cao et al., 2007), ListMLE (Xia et al., 2008), AdaRank (Xu and Li, 2007), SVM MAP (Yue et al., 2007), and Soft Rank (Taylor et al., 2008).

Apart from traditional classifiers mentioned above, neural-based ranking becomes popular again recently. Recent work includes Deep Structured Semantic Models (DSSM) (Huang et al., 2013), Deep Relevance Matching Model (DRMM) (Guo et al., 2016), Kernel-Based Neural Ranking Model (K-NRM) (Xiong et al., 2017). These neural models require fewer hand-crafted features and functions, but need more complicated methods for combining evidence and weights, and need more parameters.

# Learning to Re-rank
As introduced above, the system is given one natural language utterance which describes the intent and several decoded hypotheses which are generated from the up-streaming translator. Normally, the up-streaming translator will assign each candidate with a confidence score. The goal is to derive several qualitative criteria over the candidate hypothesis to re-rank them based on the given confidence score and any other features. In the context of machine learning, the re-ranker is learnt from a labeled training dataset and expected to generalize to unseen dataset. However, since there lacks such dataset that has ranked hypotheses, we use an intermediate medium to indirectly derive the ordering of hypothesis. Specifically, we calculate the BLEU score between each hypothesis and the gold target code, and use this BLEU score to rank each candidate hypothesis.

Figure 6 shows an overview of our TranX + Re-ranking architecture. The design of re-rank classifier is discussed from sub-section 3.1 to 3.3 and feature extraction in sub-section 3.4.

<p align="center">
<img src="img/arch.jpeg" width="600" align="middle"/>
<h4 align="center"> Figure 6. Architecture </h4>
</p>

## Pairwise Approach
In terms of pairwise re-ranking, each ranking of n hypothesis candidates is decomposed to n * (n − 1) pairs. Each pair is the ranking between two candidates and forms a training instance for the classifier. We have experimented with two different re-rankers, one is classification based Ranking support vector machine (SVM) (Herbrich, 2000) and the other is support vector regres- sion (SVR) (Drucker et al., 1997).

### Ranking SVM
Figure 1 gives an example of ranking problem. Assume there are four ranks and the weight vector w corresponds to the linear function f(x) = ⟨w, x⟩ which can score and rank the candidates. Using function f to rank candidates is analogous to projecting candidates into a vector and sorting them according to the projection on the vector.

<p align="center">
<img src="img/ranking_problem.svg" width="400" align="middle"/>
<h4 align="center"> Figure 1. Ranking Problem </h4>
</p>

Figure 2 shows that the ranking problem in Figure 1 can be converted to linear SVM classification problem. The differences between feature vector representations of any pair among the candidates are treated as new data point represented in the form of feature vector. Further more, the label to each new data point is therefore set as:

<p align="center">
<img src="img/math_1.png" width="300" align="middle"/>
</p>

For example, h1 − h2 and h2 − h1 are corresponding positive and negative samples respectively. The weight vector w of the SVM classifier corresponds to the ranking function. The Ranking SVM can be formalized as quadratic programming:

<p align="center">
<img src="img/math_2.png" width="300" align="middle"/>
</p>

where

<p align="center">
<img src="img/math_3.png" width="200" align="middle"/>
</p>

where $h_i^{(1)}$ and $h_i^{(2)}$ are the feature vectors in a pair, $\left \| \cdot  \right \|$ denotes $L_2$ norm, m is the total number of training samples, and $C > 0$ is a coefficient. It is equivalent to the non-constrained optimization function which minimizes the regularized hinge loss function:

<p align="center">
<img src="img/math_4.png" width="300" align="middle"/>
</p>

where $\lambda = \frac{1}{2C}$ (Li, 2011b).

After training the binary classifier, the simplest way to re-rank each candidate is by the number of other candidates it beats. For n hypothesis candidates in total, the rank ri of hypothesis hi is:

<p align="center">
<img src="img/math_5.png" width="200" align="middle"/>
</p>

The intuition is that the hypothesis which wins the most pairwise comparisons should be considered as the overall winner.

One of the problems seen in previous work (Avramidis, 2012) is that the aforementioned scoring would result in ties easily if the re-ranker cannot distinguish the pair, especially in the case where each pair creates two training samples (i.e., one positive and one negative). This observation encourages us to use SVR approach.

### SVR
One motivation for using regression-based re-ranker is that not all of binary classification decisions are equally important. The hypothesis candidate that has the same number of wins with higher confidence should be re-ranked higher than that of lower confidence. In the context of Ranking SVM, the simple binary decision loses this signal. Therefore, for regression-based pair-wise re-ranking, the rank ri of hypothesis hi is:

<p align="center">
<img src="img/math_6.png" width="250" align="middle"/>
</p>

where $\Delta_{i,j}$ is the output of the regression ranged from 0 to 1 which evaluates rank margin between two hypotheses - 1 corresponds to absolutely ranked higher and 0 corresponds to absolutely ranked lower, analogous to binary decision in Ranking SVM. In detail, the label of each pair is set to:

<p align="center">
<img src="img/math_7.png" width="250" align="middle"/>
</p>

where si > sj, where si is the BLEU score between hi and gold code. This normalized score difference quantifies how much hi should be ranked above hj .